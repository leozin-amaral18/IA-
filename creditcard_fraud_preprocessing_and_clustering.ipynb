Código 

# 0) Instalação (caso necessário)
# !pip install scikit-learn imbalanced-learn pandas matplotlib seaborn minisom

# 1) Imports
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from imblearn.over_sampling import SMOTE
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, adjusted_rand_score
from sklearn.decomposition import PCA
from sklearn.covariance import EllipticEnvelope
from sklearn.ensemble import IsolationForest

# For SOM
# from minisom import MiniSom

# 2) Carregar dados
PATH = 'creditcard.csv'  # caminho relativo; altere se necessário
if not os.path.exists(PATH):
    raise FileNotFoundError(f"Coloque o arquivo creditcard.csv no diretório ou ajuste PATH = '{PATH}'")

df = pd.read_csv(PATH)
print(df.shape)
print(df['Class'].value_counts())

# 3) Visualização da base de dados (EDA rápida)
print(df.info())
print(df.describe().T)

# Visualizações recomendadas (executar interativamente):
# sns.countplot(x='Class', data=df)
# plt.show()
# sns.histplot(df['Amount'], bins=100)
# plt.show()

# 4) Verificação e tratamento de valores ausentes
print('Missing per column:\n', df.isnull().sum())
# no dataset original não há missing, mas deixamos a checagem

# 5) Detecção e eliminação de redundância e inconsistência
# Duplicatas
dups = df.duplicated().sum()
print('Duplicatas:', dups)
if dups > 0:
    df = df.drop_duplicates().reset_index(drop=True)

# 6) Detecção e tratamento de outliers
# Estratégias possíveis: IQR trimming, IsolationForest, EllipticEnvelope.
# Como exemplo, aplicamos IsolationForest para marcar outliers (usado apenas como análise)
iso = IsolationForest(contamination=0.01, random_state=42)
cols_num = ['Time','Amount'] + [c for c in df.columns if c.startswith('V')]
iso_preds = iso.fit_predict(df[cols_num])
# iso_preds == -1 são outliers
print('Outliers (IsolationForest):', (iso_preds==-1).sum())

# Optionally remove outliers:
# df_clean = df[iso_preds==1].copy()
# Aqui mantemos inicialmente todos os dados e experimentamos modelos com/sem remoção

# 7) Normalização / Padronização
scaler = StandardScaler()
df_scaled = df.copy()
df_scaled[['Amount','Time']] = scaler.fit_transform(df_scaled[['Amount','Time']])

# 8) Análise de correlação e multicolinearidade
corr = df_scaled.corr()
# plt.figure(figsize=(12,10)); sns.heatmap(corr, cmap='coolwarm', vmin=-1, vmax=1); plt.show()

# VIF (Variance Inflation Factor) para multicolinearidade (apenas exemplo)
from statsmodels.stats.outliers_influence import variance_inflation_factor
X = df_scaled[[c for c in df_scaled.columns if c!='Class']]
# calcular VIF pode ser pesado com muitas features; usar uma amostra
X_sample = X.sample(min(5000, len(X)), random_state=42)
vif_data = pd.DataFrame()
vif_data['feature'] = X_sample.columns
vif_data['VIF'] = [variance_inflation_factor(X_sample.values, i) for i in range(X_sample.shape[1])]
print(vif_data.sort_values('VIF', ascending=False).head())

# 9) Codificação de variáveis
# Neste dataset não há variáveis categóricas (exceto Class) — portanto não há One-Hot necessário.

# 10) Balanceamento da classe (aplicado apenas no conjunto de treino para evitar data leakage)
X = df_scaled.drop('Class', axis=1)
y = df_scaled['Class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
print('Treino:', X_train.shape, 'Teste:', X_test.shape)

# Baseline: treinar LogisticRegression sem balanceamento (apenas para comparação)
lr = LogisticRegression(max_iter=1000, random_state=42)
lr.fit(X_train, y_train)
y_pred_proba = lr.predict_proba(X_test)[:,1]
print('AUC (baseline):', roc_auc_score(y_test, y_pred_proba))

# Agora aplicar SMOTE no conjunto de treino
sm = SMOTE(random_state=42)
X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)
print('After SMOTE', pd.Series(y_train_sm).value_counts())

# Treinar modelo após balanceamento
lr2 = LogisticRegression(max_iter=1000, random_state=42)
lr2.fit(X_train_sm, y_train_sm)
y_pred_proba2 = lr2.predict_proba(X_test)[:,1]
print('AUC (após SMOTE):', roc_auc_score(y_test, y_pred_proba2))

# Métricas adicionais (threshold default 0.5 — considerar operar em pontos da curva PR)
from sklearn.metrics import precision_recall_curve
thr = 0.5
y_pred2 = (y_pred_proba2 >= thr).astype(int)
print(classification_report(y_test, y_pred2, digits=4))

# 11) Comparar modelos (LogReg e RandomForest)
rf = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42)
rf.fit(X_train, y_train)
print('RF AUC (sem SMOTE):', roc_auc_score(y_test, rf.predict_proba(X_test)[:,1]))

rf2 = RandomForestClassifier(n_estimators=200, random_state=42)
rf2.fit(X_train_sm, y_train_sm)
print('RF AUC (com SMOTE):', roc_auc_score(y_test, rf2.predict_proba(X_test)[:,1]))

# 12) Agrupamento (remover Class)
X_cluster = X.copy()
# padronizar antes de clusterizar
sc = StandardScaler()
Xc = sc.fit_transform(X_cluster)

# KMeans (k=2)
kmeans = KMeans(n_clusters=2, random_state=42)
labels_k = kmeans.fit_predict(Xc)
print('KMeans silhouette:', silhouette_score(Xc, labels_k))
print('KMeans ARI vs Class:', adjusted_rand_score(y, labels_k))

# DBSCAN (exemplo de eps; eps deve ser ajustado)
db = DBSCAN(eps=2.0, min_samples=5)
labels_db = db.fit_predict(Xc)
# filtrar ruído para métricas de silhueta (silhouette exige >=2 clusters)
unique_db = set(labels_db)
print('DBSCAN clusters (including -1 noise):', unique_db)
if len([c for c in unique_db if c!=-1]) >= 2:
    print('DBSCAN silhouette:', silhouette_score(Xc[labels_db!=-1], labels_db[labels_db!=-1]))
print('DBSCAN ARI vs Class:', adjusted_rand_score(y, labels_db))

# SOM (Self-Organizing Map) — usando MiniSom (descomente se instalado)
# from minisom import MiniSom
# som = MiniSom(x=10, y=10, input_len=Xc.shape[1], sigma=1.0, learning_rate=0.5, random_seed=42)
# som.train_random(Xc, 100)
# # mapear cada amostra ao neurônio vencedor e construir rótulos de cluster (por exemplo, agrupando neurons em 2 clusters via KMeans sobre pesos)

# Avaliação resumida de métricas de cluster
from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score
print('KMeans CH:', calinski_harabasz_score(Xc, labels_k), 'DB:', davies_bouldin_score(Xc, labels_k))

# Salvar resultados e relatórios
# df_results = pd.DataFrame({...})
